{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nfrom pyspark.sql import functions as fn\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom pylab import *"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = spark.read.csv('/FileStore/tables/GDA_DATA.csv', header = True)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["display(df.limit(200))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["###Total number of rides"],"metadata":{}},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["###Total number of subscribers and customers"],"metadata":{}},{"cell_type":"code","source":["pd_types = df.groupBy('Subscriber Type').agg(fn.count('*').alias('rides')).toPandas()\n\npie(pd_types['rides'], labels=pd_types['Subscriber Type'],\n                autopct='%1.1f%%', shadow=True, startangle=90)\nlegend(labels=pd_types['Subscriber Type'])\ndisplay(show())\n\n# display(df.groupBy('Subscriber Type').agg(fn.count('*').alias('rides')))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["###Total time utilized by subscribers and customers in hours"],"metadata":{}},{"cell_type":"code","source":["display(df.groupBy('Subscriber Type').agg(fn.sum('Duration')/3600))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["###Average time utilized by subscribers and customers in minutes"],"metadata":{}},{"cell_type":"code","source":["display(df.groupBy('Subscriber Type').agg(fn.avg('Duration')/60))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["The observation is interesting.. The average ride time utlization time is much more for customers than subscribers. This is true even if the number of subscribers are much more than cuctomers.\nThere are 4 times more subscribers than customers, but the average ride time utilization of customers is 6 times that of subscribers\nIn terms of percentage, numeber of subscribers are 394% greater than customers, but average ride time of customers is 577% more than subscribers.\n\n###Recommendation:\nIt can be seem that customers are lese frequent riders, but usually ride for more time.\nAn attempt should be made to convert these customers to subscribers.\nFo example, there should be attractive subscription plans for a a customer, who probably rides only on Weekends forlonger time and try to convert himt to subscriber.\nThe average ride time pf subsccriber should be increased."],"metadata":{}},{"cell_type":"markdown","source":["###Most popular start stations by numer of rides taken from those stations"],"metadata":{}},{"cell_type":"code","source":["display(df.groupBy('Start Station').agg(fn.count('*').alias('rides'), fn.countDistinct('Bike #').alias('bike_count')).orderBy('rides', ascending = False).limit(20))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#####Number of rides and bikes at those staions in decreasing order"],"metadata":{}},{"cell_type":"code","source":["display(df.groupBy('End Station').agg(fn.count('*').alias('rides'), fn.countDistinct('Bike #').alias('bike_count')).orderBy('rides', ascending = False))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["##Observation\nIt can be seen that, the San Francisco Cal train station is the most popular station for people to pick bikes from. There are a total of 392 bikes at the station. \nThe second most popular station is Harry Bridges Plaza, it also has around same number od bikes.\nHowever, the number of rides at SF Caltrain station is 4500 more than Harry Bridges Plaza.\nThe bike to ride ratio at SF Caltrain St is 1:33 and at Harry Bridges Plaza is 1:21\nFor the 10th most popluar station it is 1:13\n\n###Recommendation 1\nAddress the bike to rides ratio.\nIf there are 390 bikes for 8300 rides at Harry Bridges Plaza and 386 bikes for 5240 rides at 4th St Market, the number of bikes for SF Caltrain St should be much more than 390 for almost 13,000 rides.\n\nNumber of bikes from lesser popluar stations should be transferred to more poplular stations such as SF Caltrain.\nThis will ensure that more rides are taken from more popluar stations, but the number of rides from lesser poplular stations will not decrease, as an optimum bike to rides ratio will be maintained.\n\n###Recommendation 2\nStation such as Temporary Transbay Terminal (Howard at Beale) has many big organizations such as Gap, Twilio, BlackRock, UCB surrounding it, but the utilization of bikes is very less as comapred to stations such as SF Caltrain.\nThese organizations are very good potential customers. So, there ough to be a marketing strategy to promote bikes are attract these customers. Number of rides should be increased from 5000 odd to 12000. Similar strategy should be applied to similar locations."],"metadata":{}},{"cell_type":"markdown","source":["###Most popular end stations by number of rides reaching to that station"],"metadata":{}},{"cell_type":"code","source":["display(df.groupBy('End Station').agg(fn.count('*').alias('rides')).orderBy('rides', ascending = False))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["###Most popular Routes by number of rides taken"],"metadata":{}},{"cell_type":"code","source":["display(df.groupBy('Start Station', 'End Station').agg(fn.count('*').alias('rides')).orderBy('rides', ascending = False))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["###Which area(zipcode) has the most number of rides"],"metadata":{}},{"cell_type":"markdown","source":["Remove all the rows which have no Zipcode"],"metadata":{}},{"cell_type":"code","source":["df.where((fn.col('Zip Code') == 'nil') | (fn.col('Zip Code') == '') | (fn.col('Zip Code') == ' ') | (fn.col('Zip Code') == None)).count()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Data Cleaning: There are 4033 rides with no zipcodes"],"metadata":{}},{"cell_type":"code","source":["df_zip = df.where((fn.col('Zip Code') != 'nil') | (fn.col('Zip Code') != '') | (fn.col('Zip Code') != ' ') | (fn.col('Zip Code') != None))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["display(df_zip.groupBy('Zip Code').agg(fn.count('*').alias('rides')).orderBy('rides', ascending = False).limit(20))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["###Number of rides and hours spent grouped by the area(zipcode)"],"metadata":{}},{"cell_type":"code","source":["display(df_zip.groupBy('Zip Code').agg(fn.count('*').alias('rides'), (fn.sum('Duration')/3600).alias('hours')).orderBy('rides', ascending = False).limit(20))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["***From above, it is observed that, number of rides at a partiular place corresponds to number of hours. So number of rides at a particular area is directly proportional to the hours spent***"],"metadata":{}},{"cell_type":"markdown","source":["###Relation between number of bikes and number of new users for an area"],"metadata":{}},{"cell_type":"code","source":["df_subscriber = df.where(fn.col('Subscriber Type') == 'Subscriber')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["display(df_subscriber.groupBy('Zip Code').agg(fn.count('*').alias('no_of_subscriber'), fn.countDistinct('Bike #').alias('no_of_bikes')).orderBy('no_of_bikes', ascending = False).limit(20))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["d\n###Observation\n\nIt can be seen from the above table, that area with zip code 94107 has 15655 subscribers and 542 bikes.\nHowever, area with zip code 94110 has alomost same number of bikes but only 1571 subscribers join from that area.\nThus there is non-uniformity in the distribution of bikes as per the new users. \nFor zip code 94110, the bike to new users ratio is 1:3, for every three new users there is 1 bike.\nHowever, for zip code 94107, for every 28 new users there is 1 bike.\n###Recommendation\nMore the subscribers more should be the number of bikes.\nThere should be a uniformity in the ratio of subscribers and bikes. \nPerhaphs, the bikes from area with zip code 94103 should be moved to an area where people are likely to subscribe more."],"metadata":{}},{"cell_type":"markdown","source":["##Find which hour is the busiest"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import types\ndef get_time(date):\n  return int(date.split()[1].split(':')[0])"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["get_time_udf = fn.udf(get_time, types.IntegerType())\ndf_start_hour = df.select('*', get_time_udf('Start Date').alias('start_hour'))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["display(df_start_hour.groupBy('start_hour').agg(fn.count('*').alias('rides')).orderBy('start_hour', ascending = True))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["###Observation:\nAs 8am, 9am and 5pm, 6pm are the most busiest times, it is evident that office employees are the largest consumer base.\nAn attempt should be made to attract these customers.\nFrom above observation it is seen that not many people opt for rides around Temporary Transbay Terminal (Howard at Beale) has many big organizations such as Gap, Twilio, BlackRock, UCB surrounding it"],"metadata":{}},{"cell_type":"markdown","source":["##Find which month is the busiest"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import types\ndef get_month(date):\n  return int(date.split()[0].split('/')[0])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["get_month_udf = fn.udf(get_month, types.IntegerType())\ndf_month = df.select('*', get_month_udf('Start Date').alias('month'))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["display(df_month.groupBy('month').agg(fn.count('*').alias('rides')).orderBy('month', ascending = True))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["##Observation\nThere has been a decrease in number of rides from July to August\nFor san franciso, the avg rainfall decreases from March to May, with June an july being peak sunny periods. This corresponds to the data which shows that bumber of rides increases as rainfall decrases.\nRainFall starts increasing from July onwards, which is again depicted in the data as number of rides decreases.\nHence avg rainfall is inversely proprtional to number of rides taken.**"],"metadata":{}},{"cell_type":"markdown","source":["###Number of start and end stations at each zipcode"],"metadata":{}},{"cell_type":"code","source":["display(df_zip.groupBy('Zip Code').agg(fn.countDistinct('Start Station').alias('no_of_start_startions'), fn.countDistinct('End Station').alias('no_of_end_startions')).orderBy('no_of_start_startions', ascending = False).limit(20))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["####Number of subscribers as per zip code"],"metadata":{}},{"cell_type":"code","source":["display(df_subscriber.groupBy('Zip Code').agg(fn.count('*').alias('rides')).orderBy('rides', ascending = False).limit(20))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["###Find Lat long of start station and end station"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import types\nTy = types.ArrayType(types.StructType([types.StructField('start_lat', types.FloatType()),\n                                      types.StructField('start_long', types.FloatType()),\n                                      types.StructField('end_lat', types.FloatType()),\n                                      types.StructField('end_long', types.FloatType())]))"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["def get_lat_long(start_station, end_station):\n  \n  ret_list = []\n  \n  if start_station and end_station:\n\n    response_start = requests.get('https://maps.googleapis.com/maps/api/geocode/json?address=' + start_station.replace(' ', '+') + '+CA,+USA')\n  \n    response_end = requests.get('https://maps.googleapis.com/maps/api/geocode/json?address=' + end_station.replace(' ', '+') + '+CA,+USA')\n    \n    if response_start is not None and response_end is not None:\n  \n      d = {'start_lat': response_start.json()['results'][0]['geometry']['location']['lat'] if response_start.json()['results'][0]['geometry']['location']['lat'] else None,\n       'start_long': response_start.json()['results'][0]['geometry']['location']['lng'] if response_start.json()['results'][0]['geometry']['location']['lng'] else None,\n       'end_lat': response_end.json()['results'][0]['geometry']['location']['lat'] if response_end.json()['results'][0]['geometry']['location']['lat'] else None,\n       'end_long': response_end.json()['results'][0]['geometry']['location']['lng'] if response_end.json()['results'][0]['geometry']['location']['lng'] else None}\n  \n    ret_list.append(d)\n    return ret_list"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["get_lat_long_udf = fn.udf(get_lat_long, Ty)\ndf_ll = df.select('*', get_lat_long_udf('Start Station', 'End Station').alias('lat_long'))\ndf_test = df_ll.select('*', fn.explode('lat_long').alias('lat_lng')).select('*', 'lat_lng.*')"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["**df_test dataframe contains latitudes and longitudes of start and end stations**"],"metadata":{}},{"cell_type":"markdown","source":["###Group Days of week according to the rides"],"metadata":{}},{"cell_type":"code","source":["df_days = spark.read.csv('/FileStore/tables/day_rides.csv', header = True)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["display(df_days.groupBy('day_of_week').agg(fn.count('*').alias('rides')))"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["###Group rides by date and time"],"metadata":{}},{"cell_type":"code","source":["def get_date(start_date):\n  return str(start_date.split()[0])"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["df_p = df.toPandas()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["df_p.columns"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["df_p['start_date'] = pd.to_datetime(df_p['Start Date'])\ndf_p = df_p.sort('start_date')"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["df_date = sqlContext.createDataFrame(df_p)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["df_dates_p = (df_date.groupBy('start_date').agg(fn.count('*').alias('rides'))).toPandas()"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["df_dates_p['start_date'] = pd.to_datetime(df_dates_p.start_date)\ndf_dates_p = df_dates_p.sort('start_date')\ndf_dates_p['start_date'] = df_dates_p['start_date'].astype(str)\ndf_dates = sqlContext.createDataFrame(df_dates_p)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["#####Following graph contains the rides taken at every unique time"],"metadata":{}},{"cell_type":"code","source":["display(df_dates)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["##Findings and Analysis by City Names"],"metadata":{}},{"cell_type":"code","source":["from uszipcode import ZipcodeSearchEngine\ndf = df.dropna() \nsearch = ZipcodeSearchEngine()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["def zco(x):\n    if pd.isnull(x) or x == 'nil' or x == 'Nil':\n        return None\n    else:\n        if '-' in x:\n            city = search.by_zipcode((x.split('-')[0]))['City']\n        else:            \n            city = search.by_zipcode((x))['City']\n            if city == NoneType:\n                return None\n        return city"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["df['City'] = df['Zip Code'].apply(zco)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["The csv files with cities was put back on the cluster server"],"metadata":{}},{"cell_type":"code","source":["df_city = spark.read.csv('/FileStore/tables/rides_city.csv', header = True)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["df_city = df_city.where(fn.col('City') != 'null')"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["####Distribution of Rides as per City"],"metadata":{}},{"cell_type":"code","source":["display(df_city.groupby('City').agg(fn.count('*').alias('rides')).orderBy('rides', ascending = False))"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["df_city_p = df_city.toPandas()"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["def get_date(start_date):\n  return str(start_date.split()[0])"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["df_city_p['start_date'] = df_city_p['Start Date'].apply(get_date)"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["df_city_p['start_date'] = pd.to_datetime(df_city_p['start_date'])\ndf_city_p = df_city_p.sort('start_date')"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["df_date_city = sqlContext.createDataFrame(df_city_p)"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["####Customer and Subscriber rides distribution in San Francisco"],"metadata":{}},{"cell_type":"code","source":["df_SF_sub = df_date_city.where(fn.col('City') == 'San Francisco').where(fn.col('Subscriber Type') == 'Subscriber')"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["df_SF_sub_p = (df_SF_sub.groupBy('start_date').agg(fn.count('*').alias('rides'))).toPandas()\ndf_SF_sub_p['start_date'] = pd.to_datetime(df_SF_sub_p.start_date)\ndf_SF_sub_p = df_SF_sub_p.sort('start_date')\ndf_SF_sub_p['start_date'] = df_SF_sub_p['start_date'].astype(str)\ndf_SF_sub_date = sqlContext.createDataFrame(df_SF_sub_p)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["#####Subscriber rides distribution in San Francisco"],"metadata":{}},{"cell_type":"code","source":["display(df_SF_sub_date)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["df_SF_cus = df_date_city.where(fn.col('City') == 'San Francisco').where(fn.col('Subscriber Type') == 'Customer')\ndf_SF_cus_p = (df_SF_cus.groupBy('start_date').agg(fn.count('*').alias('rides'))).toPandas()\ndf_SF_cus_p['start_date'] = pd.to_datetime(df_SF_cus_p.start_date)\ndf_SF_cus_p = df_SF_cus_p.sort('start_date')\ndf_SF_cus_p['start_date'] = df_SF_cus_p['start_date'].astype(str)\ndf_SF_cus_date = sqlContext.createDataFrame(df_SF_cus_p)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["#####Customer rides distribution in San Francisco"],"metadata":{}},{"cell_type":"code","source":["display(df_SF_cus_date)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":["####Subscriber and Customer rides distribution in San Jose"],"metadata":{}},{"cell_type":"code","source":["df_SJ_sub = df_date_city.where(fn.col('City') == 'San Jose').where(fn.col('Subscriber Type') == 'Subscriber')\ndf_SJ_sub_p = (df_SJ_sub.groupBy('start_date').agg(fn.count('*').alias('rides'))).toPandas()\ndf_SJ_sub_p['start_date'] = pd.to_datetime(df_SJ_sub_p.start_date)\ndf_SJ_sub_p = df_SJ_sub_p.sort('start_date')\ndf_SJ_sub_p['start_date'] = df_SJ_sub_p['start_date'].astype(str)\ndf_SJ_sub_date = sqlContext.createDataFrame(df_SJ_sub_p)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["df_SJ_cus = df_date_city.where(fn.col('City') == 'San Jose').where(fn.col('Subscriber Type') == 'Customer')\ndf_SJ_cus_p = (df_SJ_cus.groupBy('start_date').agg(fn.count('*').alias('rides'))).toPandas()\ndf_SJ_cus_p['start_date'] = pd.to_datetime(df_SJ_cus_p.start_date)\ndf_SJ_cus_p = df_SJ_cus_p.sort('start_date')\ndf_SJ_cus_p['start_date'] = df_SJ_cus_p['start_date'].astype(str)\ndf_SJ_cus_date = sqlContext.createDataFrame(df_SJ_cus_p)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["df_SJ_cus_date = df_SJ_cus_date.withColumnRenamed('rides', 'rides_cus')\ndf_SJ_rides = df_SJ_cus_date.join(df_SJ_sub_date, df_SJ_cus_date.start_date == df_SJ_sub_date.start_date).select(df_SJ_cus_date.start_date, df_SJ_cus_date.rides_cus, df_SJ_sub_date.rides)\ndf_SJ_rides_p = df_SJ_rides.toPandas()\ndf_SJ_rides_p['start_date'] = pd.to_datetime(df_SJ_rides_p.start_date)\ndf_SJ_rides_p = df_SJ_rides_p.sort('start_date')\ndf_SJ_rides_p['start_date'] = df_SJ_rides_p['start_date'].astype(str)\ndf_SJ_rides_sp = sqlContext.createDataFrame(df_SJ_rides_p)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["####Subscriber vs Customer rides distribution in San Jose"],"metadata":{}},{"cell_type":"code","source":["display(df_SJ_rides_sp)"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":["####Subscriber vs Customer rides distribution in Oakland"],"metadata":{}},{"cell_type":"code","source":["df_O_sub = df_date_city.where(fn.col('City') == 'Oakland').where(fn.col('Subscriber Type') == 'Subscriber')\ndf_O_sub_date = df_O_sub.groupBy('start_date').agg(fn.count('*').alias('rides_sub'))\ndf_O_cus = df_date_city.where(fn.col('City') == 'Oakland').where(fn.col('Subscriber Type') == 'Customer')\ndf_O_cus_date = df_SJ_cus.groupBy('start_date').agg(fn.count('*').alias('rides_cus'))\ndf_O_rides = df_O_cus_date.join(df_O_sub_date, df_O_cus_date.start_date == df_O_sub_date.start_date).select(df_O_cus_date.start_date, df_O_cus_date.rides_cus, df_O_sub_date.rides_sub)\n\ndf_O_rides_p = df_O_rides.toPandas()\ndf_O_rides_p['start_date'] = pd.to_datetime(df_O_rides_p.start_date)\ndf_O_rides_p = df_O_rides_p.sort('start_date')\ndf_O_rides_p['start_date'] = df_O_rides_p['start_date'].astype(str)\ndf_O_rides_sp = sqlContext.createDataFrame(df_O_rides_p)"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["display(df_O_rides_sp)"],"metadata":{},"outputs":[],"execution_count":97}],"metadata":{"name":"uber","notebookId":2115429688883780},"nbformat":4,"nbformat_minor":0}
